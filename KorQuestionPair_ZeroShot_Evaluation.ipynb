{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad8ef277-c97f-434c-87a4-4720702318aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aicombined/miniconda3/envs/poc/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-06-17 20:44:27,969\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import numpy as np\n",
    "import glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b01b03-794b-471e-afd4-ae0f9cc82939",
   "metadata": {},
   "source": [
    "- Instruction Formatting 전처리 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cb49bc8-7de4-4e36-bb37-616f362bb143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(x):\n",
    "    return f\"###입력:{x}\\n\\n###출력:\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6e1b15-5ce1-4118-949b-ca85d65bd614",
   "metadata": {},
   "source": [
    "- merge_16bit로 저장된 모델 weight의 폴더를 이용하여 llm를 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c02c57ea-a762-4d3f-ad3a-53f1d00b70e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-17 20:44:28 llm_engine.py:100] Initializing an LLM engine (v0.4.2) with config: model='huggingface_merged_weights/SOLAR/', speculative_config=None, tokenizer='huggingface_merged_weights/SOLAR/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=huggingface_merged_weights/SOLAR/)\n",
      "INFO 06-17 20:44:28 utils.py:660] Found nccl from library /home/aicombined/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "INFO 06-17 20:44:29 selector.py:81] Cannot use FlashAttention-2 backend because the flash_attn package is not found. Please install it for better performance.\n",
      "INFO 06-17 20:44:29 selector.py:32] Using XFormers backend.\n",
      "INFO 06-17 20:44:42 model_runner.py:175] Loading model weights took 19.9939 GB\n",
      "INFO 06-17 20:44:44 gpu_executor.py:114] # GPU blocks: 7431, # CPU blocks: 1365\n",
      "INFO 06-17 20:44:47 model_runner.py:937] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 06-17 20:44:47 model_runner.py:941] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 06-17 20:44:56 model_runner.py:1017] Graph capturing finished in 8 secs.\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=\"huggingface_merged_weights/SOLAR/\", enable_lora=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa017a6e-b9b0-4140-bbe0-63ab733f8b03",
   "metadata": {},
   "source": [
    "- 자연어 생성을 위한 그리드 서치 디코딩 파라미터 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91c5d470-5e2a-4279-8b93-50425c0df634",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(max_tokens=256, temperature=0.5, top_k=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123f187b-89b6-4e4b-8061-fcedd086bdca",
   "metadata": {},
   "source": [
    "- korea_hate_speech 데이터셋 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ac7d70c-141b-4676-b64b-7482bd20f6b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datapaths = glob.glob('./evaluation_dataset/04_kor_question_pair/*.csv')\n",
    "sentences1 = []\n",
    "sentences2 = []\n",
    "_labels = []\n",
    "\n",
    "for path in datapaths:\n",
    "    ques_pair_data = pd.read_csv(path)\n",
    "    question1 = list(ques_pair_data['question1'].values)\n",
    "    question2 = list(ques_pair_data['question2'].values)\n",
    "    label = list(ques_pair_data['is_duplicate'].values)\n",
    "\n",
    "    sentences1.extend(question1)\n",
    "    sentences2.extend(question2)\n",
    "    _labels.extend(label)\n",
    "\n",
    "# label 별도 처리\n",
    "labels = []\n",
    "for label  in _labels:\n",
    "    if label == 1:\n",
    "        labels.append('부정')\n",
    "    elif label == 0:\n",
    "        labels.append('긍정')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b96a24c3-8feb-4fd8-83fc-3b3dbdea7c49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('21살의 사랑에 대해', '사랑을 노력한다는게 말이 되나요?', '부정')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences1[0], sentences2[0], labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b695ef-e63e-4262-a120-9497ddd4417a",
   "metadata": {},
   "source": [
    "- 입력 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0544cdf6-0078-4a55-b830-1ffaef6085a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"문장1과 문장2의 의미가 같으면 긍정, 의미가 같지 않으면 부정으로 대답하세요. 의미가 같다는 말은 문장의 길이, 어순 또는 사용하는 단어가 달라졌음에도 불구하고 두 문장이 같은 의미임을 나타냅니다.\\n\\n\"\"\"\n",
    "texts = []\n",
    "\n",
    "for sentence1, sentence2, label in zip(sentences1, sentences2, labels):\n",
    "    instruction = \"문장 1 : {} \\n 문장 2 : {}\".format(sentence1, sentence2)\n",
    "    texts.append(preprocess_text(system_prompt + instruction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182d7249-e686-4d30-82c7-4fd4c6c9687f",
   "metadata": {},
   "source": [
    "- inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce96455d-1a0d-47ec-9c2c-1783c85355ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 7576/7576 [05:29<00:00, 23.02it/s]\n"
     ]
    }
   ],
   "source": [
    "outputs = llm.generate(texts, sampling_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af87919b-c6f5-46e9-a785-39c2c5986970",
   "metadata": {},
   "source": [
    "- 생성 결과 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f948b099-b9c3-4bd3-b8c2-78211a4a90cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds = []\n",
    "for output in outputs:\n",
    "    generated_text = output.outputs[0].text\n",
    "    preds.append(generated_text.split('$&%')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12f36b05-2d10-49a5-867d-6c24d5b4e1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_processed = []\n",
    "labels_processed = []\n",
    "\n",
    "for y_pred, y_true in zip(preds, labels):\n",
    "    if y_pred == '긍정' or y_pred == '부정':\n",
    "        preds_processed.append(y_pred)\n",
    "        labels_processed.append(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87f86b62-fd97-4e5b-9641-acdacc6dacd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7576, 7560)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels), len(labels_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f48c858-ceb5-4139-8a7a-e73855537202",
   "metadata": {},
   "source": [
    "- zero-shot 분류 성능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "529953c6-487e-4733-8576-9e3ab13ee038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          긍정       0.77      0.89      0.83      5101\n",
      "          부정       0.66      0.46      0.54      2459\n",
      "\n",
      "    accuracy                           0.75      7560\n",
      "   macro avg       0.72      0.67      0.68      7560\n",
      "weighted avg       0.74      0.75      0.73      7560\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels_processed, preds_processed))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poc",
   "language": "python",
   "name": "poc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
